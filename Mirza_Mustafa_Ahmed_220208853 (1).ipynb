{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pwbNZ-1ekVD"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sentencepiece --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oNHVxe5f9vU"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-ur\"\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AZ2xGKrgmj9"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    # examples[\"translation\"] is a list of dicts\n",
        "    en_texts = [item[\"en\"] for item in examples[\"translation\"]]\n",
        "    ur_texts = [item[\"ur\"] for item in examples[\"translation\"]]\n",
        "\n",
        "    model_inputs = tokenizer(en_texts, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(ur_texts, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDxH6qJOjhuW"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODrg4Azxjl2v"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./en-ur-translation-model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=False,  # disable for old versions\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbAQhIU_pi3S"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model, src_sentence, src_tokenizer, tgt_tokenizer, max_len=50):\n",
        "    model.eval()\n",
        "    src = torch.tensor(src_tokenizer.encode(src_sentence)).unsqueeze(1).to(device)  # (seq_len, 1)\n",
        "    src_mask = torch.zeros(src.size(0), src.size(0), device=device).type(torch.bool)\n",
        "\n",
        "    memory = model.encoder(model.pos_encoder(model.src_embedding(src) * math.sqrt(D_MODEL)), src_mask)\n",
        "\n",
        "    ys = torch.ones(1, 1).fill_(tgt_tokenizer.pad_id).type(torch.long).to(device)  # start token (or pad_id)\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        tgt_mask = generate_square_subsequent_mask(ys.size(0)).to(device)\n",
        "        out = model.decoder(model.pos_encoder(model.tgt_embedding(ys) * math.sqrt(D_MODEL)), memory, tgt_mask)\n",
        "        out = model.fc_out(out)\n",
        "        prob = out[-1, 0].softmax(dim=-1)\n",
        "        next_word = torch.argmax(prob).item()\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == tgt_tokenizer.pad_id:  # stop if pad or eos token\n",
        "            break\n",
        "\n",
        "    translated_tokens = ys.flatten().cpu().numpy()\n",
        "    return tgt_tokenizer.decode(translated_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg4GhMfnqolI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "\n",
        "# Define device and D_MODEL\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# For Helsinki-NLP/opus-mt models, this is typically 512.\n",
        "D_MODEL = 512\n",
        "\n",
        "# Transformer examples.\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def greedy_decode(model, src_sentence, src_tokenizer, tgt_tokenizer, max_len=50):\n",
        "    model.eval()\n",
        "    # The encode method of MarianTokenizer returns a dict, need to access input_ids\n",
        "    src_tokens = src_tokenizer.encode(src_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "    # A simplified approach using the model's generate method for translation\n",
        "    input_ids = src_tokens\n",
        "    translated_tokens = model.generate(input_ids, max_length=max_len, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # The generate method returns a tensor of token ids\n",
        "    translated_tokens = translated_tokens[0].cpu().numpy()\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    # Decode needs skip_special_tokens=True to remove padding/EOS tokens from output\n",
        "    return tgt_tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
        "\n",
        "# Re-run the model and tokenizer loading (ensure you use the correct model_name from earlier cells)\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-ur\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name).to(device) # Move model to device\n",
        "\n",
        "src_tokenizer = tokenizer\n",
        "tgt_tokenizer = tokenizer\n",
        "\n",
        "# List of English test sentences\n",
        "test_sentences = [\n",
        "    \"how are you?\"\n",
        "]\n",
        "\n",
        "# Loop through test sentences and print translations\n",
        "for sentence in test_sentences:\n",
        "    # Pass the model that is on the correct device\n",
        "    translation = greedy_decode(model, sentence, src_tokenizer, tgt_tokenizer)\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"Urdu: {translation}\")\n",
        "    print(\"-\" * 40)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}