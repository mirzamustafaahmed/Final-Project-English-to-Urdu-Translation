{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pwbNZ-1ekVD"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sentencepiece --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load English-Urdu translation dataset from opus100\n",
        "dataset = load_dataset(\"opus100\", \"en-ur\", split=\"train[:5%]\")\n",
        "\n",
        "# Display a sample\n",
        "print(f\"Number of examples: {len(dataset)}\")\n",
        "print(\"Sample:\", dataset[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "FWbyhKD3fAnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-ur\"\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "1oNHVxe5f9vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # examples[\"translation\"] is a list of dicts\n",
        "    en_texts = [item[\"en\"] for item in examples[\"translation\"]]\n",
        "    ur_texts = [item[\"ur\"] for item in examples[\"translation\"]]\n",
        "\n",
        "    model_inputs = tokenizer(en_texts, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(ur_texts, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "1AZ2xGKrgmj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"translation\"]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "fPh5pl55hDE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "id": "KDxH6qJOjhuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./en-ur-translation-model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=False,  # disable for old versions\n",
        ")\n"
      ],
      "metadata": {
        "id": "ODrg4Azxjl2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1. Load your dataset (example: IMDb reviews)\n",
        "dataset = load_dataset(\"imdb\")  # or your own dataset\n",
        "\n",
        "# 2. Load your tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # replace with your model tokenizer\n",
        "\n",
        "# 3. Tokenize function for dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# 4. Apply tokenizer to dataset\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Now you have tokenized_dataset ready for training\n"
      ],
      "metadata": {
        "id": "fUwJ9SWNm9Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src_sentence, src_tokenizer, tgt_tokenizer, max_len=50):\n",
        "    model.eval()\n",
        "    src = torch.tensor(src_tokenizer.encode(src_sentence)).unsqueeze(1).to(device)  # (seq_len, 1)\n",
        "    src_mask = torch.zeros(src.size(0), src.size(0), device=device).type(torch.bool)\n",
        "\n",
        "    memory = model.encoder(model.pos_encoder(model.src_embedding(src) * math.sqrt(D_MODEL)), src_mask)\n",
        "\n",
        "    ys = torch.ones(1, 1).fill_(tgt_tokenizer.pad_id).type(torch.long).to(device)  # start token (or pad_id)\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        tgt_mask = generate_square_subsequent_mask(ys.size(0)).to(device)\n",
        "        out = model.decoder(model.pos_encoder(model.tgt_embedding(ys) * math.sqrt(D_MODEL)), memory, tgt_mask)\n",
        "        out = model.fc_out(out)\n",
        "        prob = out[-1, 0].softmax(dim=-1)\n",
        "        next_word = torch.argmax(prob).item()\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == tgt_tokenizer.pad_id:  # stop if pad or eos token\n",
        "            break\n",
        "\n",
        "    translated_tokens = ys.flatten().cpu().numpy()\n",
        "    return tgt_tokenizer.decode(translated_tokens)\n",
        "\n",
        "# Example usage:\n",
        "# translated_text = greedy_decode(model, \"How are you?\", src_tokenizer, tgt_tokenizer)\n",
        "# print(\"Translated:\", translated_text)\n"
      ],
      "metadata": {
        "id": "GbAQhIU_pi3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "import math\n",
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "\n",
        "# Define device and D_MODEL\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# D_MODEL should correspond to the hidden size of your model.\n",
        "# For Helsinki-NLP/opus-mt models, this is typically 512.\n",
        "D_MODEL = 512\n",
        "\n",
        "# Placeholder for generate_square_subsequent_mask function, which is likely\n",
        "# intended for a standard Transformer architecture.\n",
        "# The MarianMTModel uses a different internal structure, so this part of the\n",
        "# greedy_decode function might need significant modification to work correctly\n",
        "# with MarianMTModel. This implementation is a common version found in\n",
        "# Transformer examples.\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def greedy_decode(model, src_sentence, src_tokenizer, tgt_tokenizer, max_len=50):\n",
        "    model.eval()\n",
        "    # The encode method of MarianTokenizer returns a dict, need to access input_ids\n",
        "    src_tokens = src_tokenizer.encode(src_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # MarianMTModel does not use a separate encoder/decoder like this\n",
        "    # This part of the function is not compatible with MarianMTModel structure\n",
        "    # memory = model.encoder(model.pos_encoder(model.src_embedding(src) * math.sqrt(D_MODEL)), src_mask)\n",
        "\n",
        "    # Instead, for generation with MarianMTModel, you should use the model's generate method\n",
        "    # This requires significantly changing the function's approach\n",
        "\n",
        "    # A simplified approach using the model's generate method for translation\n",
        "    input_ids = src_tokens\n",
        "    translated_tokens = model.generate(input_ids, max_length=max_len, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # The generate method returns a tensor of token ids\n",
        "    translated_tokens = translated_tokens[0].cpu().numpy()\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    # Decode needs skip_special_tokens=True to remove padding/EOS tokens from output\n",
        "    return tgt_tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
        "\n",
        "# Re-run the model and tokenizer loading (ensure you use the correct model_name from earlier cells)\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-ur\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name).to(device) # Move model to device\n",
        "\n",
        "src_tokenizer = tokenizer\n",
        "tgt_tokenizer = tokenizer\n",
        "\n",
        "# List of English test sentences\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"What is your name?\",\n",
        "    \"I love learning new languages.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"Can you help me translate this?\"\n",
        "]\n",
        "\n",
        "# Loop through test sentences and print translations\n",
        "for sentence in test_sentences:\n",
        "    # Pass the model that is on the correct device\n",
        "    translation = greedy_decode(model, sentence, src_tokenizer, tgt_tokenizer)\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"Urdu: {translation}\")\n",
        "    print(\"-\" * 40)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rg4GhMfnqolI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}